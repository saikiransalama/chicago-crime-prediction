{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from folium.plugins import HeatMap\n",
    "from folium.plugins import HeatMapWithTime\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import scipy.stats as stats\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows to see what the data looks like\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of the dataframe to understand data types and non-null counts\n",
    "print(\"\\nDataframe Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for any immediate missing values\n",
    "print(\"\\nMissing Values Summary:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'Date' and 'Updated On' columns to datetime objects\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "df['Updated On'] = pd.to_datetime(df['Updated On'], format='%m/%d/%Y %I:%M:%S %p', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking conversion results\n",
    "print(\"Date conversion check:\")\n",
    "print(df[['Date', 'Updated On']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of the dataset\n",
    "print(\"Shape of dataset:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping rows with any missing values across the entire DataFrame\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of the dataset after dropping missing values\n",
    "print(\"Shape of dataset after dropping missing values:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping unnecessary columns to streamline the dataset\n",
    "x = ['ID', 'Case Number', 'IUCR', 'Description', 'FBI Code', 'Beat', 'Location']\n",
    "df.drop(columns=x, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of the dataset after dropping missing values\n",
    "print(\"Shape of dataset after dropping unecessary columns:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created additional time-based features from the 'Date' column\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df['Hour'] = df['Date'].dt.hour\n",
    "df['DayOfWeek'] = df['Date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Createing a new column 'Area Type' based on 'Location Description'\n",
    "# If 'RESIDENCE' is in the description, label it as \"Residential\", otherwise \"Non-Residential\"\n",
    "df['Area Type'] = df['Location Description'].apply(\n",
    "    lambda x: \"Residential\" if isinstance(x, str) and \"RESIDENCE\" in x.upper() else \"Non-Residential\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying changes\n",
    "print(\"Preprocessed DataFrame columns:\", df.columns.tolist())\n",
    "print(\"Preprocessed DataFrame shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows to see what the data looks like\n",
    "print(\"First 5 rows of the preprocessed dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Question 1\n",
    "## What are the most frequent crime types in Chicago, and how do they correlate with location (residential vs. commercial areas)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count overall occurrences of each crime type\n",
    "crime_counts = df['Primary Type'].value_counts().reset_index()\n",
    "crime_counts.columns = ['Primary Type', 'Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by Primary Type and Area Type\n",
    "crime_by_area = df.groupby(['Primary Type', 'Area Type']).size().reset_index(name='Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall top 10 crime types\n",
    "plt.figure(figsize=(12,6))\n",
    "top_10 = crime_counts.head(10)\n",
    "ax = sns.barplot(data=top_10, x='Primary Type', y='Count', hue='Primary Type', palette='viridis', dodge=False)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Top 10 Most Frequent Crime Types in Chicago')\n",
    "plt.xlabel('Primary Crime Type')\n",
    "plt.ylabel('Number of Crimes')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the breakdown of the top 10 crime types by Area Type\n",
    "top_10_types = top_10['Primary Type'].tolist()\n",
    "subset = crime_by_area[crime_by_area['Primary Type'].isin(top_10_types)]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(data=subset, x='Primary Type', y='Count', hue='Area Type', palette='magma')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Top 10 Crime Types by Area Type (Residential vs Non-Residential)')\n",
    "plt.xlabel('Primary Crime Type')\n",
    "plt.ylabel('Number of Crimes')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined a helper function to extract words from a text\n",
    "def extract_words(text):\n",
    "    # Using regex to extract alphanumeric words; converting to uppercase for consistency\n",
    "    return re.findall(r'\\w+', text.upper())\n",
    "\n",
    "# Initialized an empty list to hold all words\n",
    "all_words = []\n",
    "\n",
    "# Iterate over the 'Location Description' column\n",
    "for desc in df['Location Description']:\n",
    "    if isinstance(desc, str):\n",
    "        words = extract_words(desc)\n",
    "        all_words.extend(words)\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Display the 50 most common words\n",
    "top_words = word_freq.most_common(50)\n",
    "print(\"Top 50 words in 'Location Description':\")\n",
    "for word, count in top_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(text):\n",
    "    # Extracting alphanumeric words and convert to uppercase for consistency\n",
    "    return re.findall(r'\\w+', text.upper())\n",
    "\n",
    "# Defined residential keywords based on the top words observed in above code\n",
    "residential_keywords = [\"RESIDENCE\", \"APARTMENT\", \"RESIDENTIAL\", \"RESID\", \"HOME\", \"HOUSE\"]\n",
    "\n",
    "def classify_location_by_words(desc):\n",
    "    if isinstance(desc, str):\n",
    "        words = set(extract_words(desc))\n",
    "        # If any of the residential keywords are present, classify as Residential\n",
    "        if any(keyword in words for keyword in residential_keywords):\n",
    "            return \"Residential\"\n",
    "        else:\n",
    "            return \"Non-Residential\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Apply the classification function to the 'Location Description' column\n",
    "df['Area Type'] = df['Location Description'].apply(classify_location_by_words)\n",
    "\n",
    "# Display the distribution of the new classification\n",
    "print(\"Updated Area Type Distribution:\")\n",
    "print(df['Area Type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall top 10 crime types from overall counts\n",
    "crime_counts = df['Primary Type'].value_counts().reset_index()\n",
    "crime_counts.columns = ['Primary Type', 'Count']\n",
    "top_10 = crime_counts.head(10)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "ax = sns.barplot(data=top_10, x='Primary Type', y='Count', hue='Primary Type', palette='viridis', dodge=False)\n",
    "ax.set(yscale=\"log\")  # Applying log scale\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Top 10 Most Frequent Crime Types in Chicago (Log Scale)')\n",
    "plt.xlabel('Primary Crime Type')\n",
    "plt.ylabel('Number of Crimes (Log Scale)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined a function to classify time of day\n",
    "def time_of_day(hour):\n",
    "    # Defined 6 AM to 6 PM as Daytime, else Nighttime\n",
    "    return \"Daytime\" if 6 <= hour < 18 else \"Nighttime\"\n",
    "\n",
    "# Creating the TimeOfDay column\n",
    "df['TimeOfDay'] = df['Hour'].apply(time_of_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distribution\n",
    "print(\"Time of Day Distribution:\")\n",
    "print(df['TimeOfDay'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the top 10 crime types by TimeOfDay\n",
    "top_10_types = top_10['Primary Type'].tolist()\n",
    "subset_time = df[df['Primary Type'].isin(top_10_types)]\n",
    "time_grouped = subset_time.groupby(['Primary Type', 'TimeOfDay']).size().reset_index(name='Count')\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(data=time_grouped, x='Primary Type', y='Count', hue='TimeOfDay', palette='coolwarm')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Top 10 Crime Types by Time of Day (Daytime vs. Nighttime)')\n",
    "plt.xlabel('Primary Crime Type')\n",
    "plt.ylabel('Number of Crimes')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pivot table: count of crimes by District and Month\n",
    "# Making sure the District column is in a suitable format\n",
    "df['District'] = df['District'].astype(int)\n",
    "\n",
    "district_month = df.pivot_table(index='District', columns='Month', values='Primary Type', aggfunc='count', fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(district_month, cmap='YlGnBu', annot=True, fmt='d')\n",
    "plt.title('Heatmap of Crime Counts by District and Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('District')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall crime counts by Primary Type\n",
    "crime_counts = df['Primary Type'].value_counts().reset_index()\n",
    "crime_counts.columns = ['Primary Type', 'Count']\n",
    "\n",
    "# Get the overall top 10 crime types\n",
    "top_10_crimes = crime_counts.head(10)['Primary Type'].tolist()\n",
    "\n",
    "# Filtering the DataFrame for only the top 10 crime types\n",
    "df_top10 = df[df['Primary Type'].isin(top_10_crimes)]\n",
    "\n",
    "# Sampled a subset of records for plotting (to avoid overplotting)\n",
    "df_sample = df_top10.sample(n=10000, random_state=42)\n",
    "\n",
    "# Creating the scatter plot using longitude and latitude; color by crime type\n",
    "plt.figure(figsize=(12,10))\n",
    "scatter = sns.scatterplot(data=df_sample, x='Longitude', y='Latitude', \n",
    "                          hue='Primary Type', palette='tab10', alpha=0.5)\n",
    "\n",
    "plt.title('Spatial Distribution of Top 10 Crime Types in Chicago')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated overall crime counts and extract the top 10 crime types (if not already done)\n",
    "crime_counts = df['Primary Type'].value_counts().reset_index()\n",
    "crime_counts.columns = ['Primary Type', 'Count']\n",
    "top_10_crimes = crime_counts.head(10)['Primary Type'].tolist()\n",
    "\n",
    "# Filtered the DataFrame for only the top 10 crime types\n",
    "df_top10 = df[df['Primary Type'].isin(top_10_crimes)]\n",
    "\n",
    "# Sampled a subset of records for mapping to avoid performance issues\n",
    "df_sample = df_top10.sample(n=10000, random_state=42)\n",
    "\n",
    "# Set Chicago's approximate center coordinates\n",
    "map_center = [41.8781, -87.6298]\n",
    "crime_map = folium.Map(location=map_center, zoom_start=11)\n",
    "\n",
    "# Added a marker cluster to group nearby markers\n",
    "marker_cluster = MarkerCluster().add_to(crime_map)\n",
    "\n",
    "# Iterated through the sampled data and add markers with popups\n",
    "for idx, row in df_sample.iterrows():\n",
    "    # Extracted required fields\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "    crime_type = row['Primary Type']\n",
    "    time_of_day = row['TimeOfDay']\n",
    "    date_str = row['Date'].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    popup_text = f\"Crime: {crime_type}<br>Time: {time_of_day}<br>Date: {date_str}\"\n",
    "    \n",
    "    folium.Marker(\n",
    "        location=[lat, lon],\n",
    "        popup=popup_text,\n",
    "        icon=folium.Icon(color=\"blue\", icon=\"info-sign\")\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Saving the map to an HTML\n",
    "crime_map.save(\"chicago_crime_interactive_map.html\")\n",
    "\n",
    "# Display the map in Jupyter Notebook\n",
    "crime_map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a list of [latitude, longitude] pairs for all records.\n",
    "heat_data = df[['Latitude', 'Longitude']].values.tolist()\n",
    "\n",
    "# Setting up the map centered on Chicago.\n",
    "map_all = folium.Map(location=[41.8781, -87.6298], zoom_start=11)\n",
    "\n",
    "# Added a HeatMap layer with custom parameters (tweak radius and blur as needed)\n",
    "HeatMap(heat_data, radius=10, blur=15, max_zoom=1).add_to(map_all)\n",
    "\n",
    "# Saving the map to an HTML file if needed\n",
    "map_all.save(\"chicago_crime_heatmap.html\")\n",
    "\n",
    "# Display the heatmap in Jupyter Notebook\n",
    "map_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing overall crime counts and extract the top 10 crime types\n",
    "crime_counts = df['Primary Type'].value_counts().reset_index()\n",
    "crime_counts.columns = ['Primary Type', 'Count']\n",
    "top_10_crimes = crime_counts.head(10)['Primary Type'].tolist()\n",
    "\n",
    "# Created a base map centered on Chicago\n",
    "map_top10 = folium.Map(location=[41.8781, -87.6298], zoom_start=11)\n",
    "\n",
    "# Loop over each top crime type, filter the DataFrame, and add a HeatMap layer for each\n",
    "for crime in top_10_crimes:\n",
    "    df_crime = df[df['Primary Type'] == crime]\n",
    "    heat_data = df_crime[['Latitude', 'Longitude']].values.tolist()\n",
    "    \n",
    "    # Create a feature group for the current crime type\n",
    "    fg = folium.FeatureGroup(name=crime)\n",
    "    HeatMap(heat_data, radius=10, blur=15, max_zoom=1).add_to(fg)\n",
    "    fg.add_to(map_top10)\n",
    "\n",
    "# Added layer control to toggle layers on/off\n",
    "folium.LayerControl().add_to(map_top10)\n",
    "\n",
    "# Save and display the map\n",
    "map_top10.save(\"chicago_top10_crime_types_heatmap.html\")\n",
    "map_top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geting the unique crime types and their counts\n",
    "unique_crime_types = df['Primary Type'].value_counts()\n",
    "\n",
    "# Print the unique crime types sorted by frequency\n",
    "print(\"Crime Types Sorted by Frequency:\")\n",
    "print(unique_crime_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classification sets based on our reasoning:\n",
    "violent_types = {\"BATTERY\", \"ASSAULT\", \"ROBBERY\", \"CRIM SEXUAL ASSAULT\", \"CRIMINAL SEXUAL ASSAULT\", \"HOMICIDE\", \"KIDNAPPING\"}\n",
    "property_types = {\"THEFT\", \"BURGLARY\", \"MOTOR VEHICLE THEFT\", \"CRIMINAL DAMAGE\", \"ARSON\", \"DECEPTIVE PRACTICE\"}\n",
    "\n",
    "# Creating a function to classify each crime\n",
    "def categorize_crime(primary_type):\n",
    "    p_type = primary_type.upper()\n",
    "    if p_type in violent_types:\n",
    "        return \"Violent\"\n",
    "    elif p_type in property_types:\n",
    "        return \"Property\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "# Applying the classification function to create a new column\n",
    "df['Crime Category'] = df['Primary Type'].apply(categorize_crime)\n",
    "\n",
    "# Print the distribution of the new crime categories\n",
    "print(\"\\nCrime Category Distribution:\")\n",
    "print(df['Crime Category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered monthly counts for only Violent and Property crimes\n",
    "filtered_counts = df[df['Crime Category'].isin(['Violent', 'Property'])]\n",
    "\n",
    "# Aggregated monthly crime counts by category\n",
    "monthly_counts = filtered_counts.groupby(['Month', 'Crime Category']).size().reset_index(name='Count')\n",
    "\n",
    "# Ploting a line chart to visualize seasonal trends\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(data=monthly_counts, x='Month', y='Count', hue='Crime Category', marker='o')\n",
    "plt.title('Monthly Crime Counts by Category (Violent vs. Property)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Crimes')\n",
    "plt.xticks(range(1, 13))\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the dataset for Violent and Property crimes only\n",
    "df_vp = df[df['Crime Category'].isin(['Violent', 'Property'])]\n",
    "\n",
    "# Violent Crimes Heatmap\n",
    "violent_pivot = df_vp[df_vp['Crime Category'] == 'Violent'] \\\n",
    "    .groupby(['Year', 'Month']).size().unstack(fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(violent_pivot, annot=True, fmt='d', cmap='Reds')\n",
    "plt.title('Heatmap of Violent Crimes by Year and Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Year')\n",
    "plt.show()\n",
    "\n",
    "# Property Crimes Heatmap\n",
    "property_pivot = df_vp[df_vp['Crime Category'] == 'Property'] \\\n",
    "    .groupby(['Year', 'Month']).size().unstack(fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(property_pivot, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Heatmap of Property Crimes by Year and Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated hourly counts for violent and property crimes\n",
    "hourly_counts = df_vp.groupby(['Hour', 'Crime Category']).size().reset_index(name='Count')\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.lineplot(data=hourly_counts, x='Hour', y='Count', hue='Crime Category', marker='o')\n",
    "plt.title('Hourly Crime Counts by Category (Violent vs. Property)')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Number of Crimes')\n",
    "plt.xticks(range(0,24))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for violent crimes and then for selected subtypes\n",
    "violent_sub = df[df['Crime Category'] == 'Violent']\n",
    "selected_violent = violent_sub[violent_sub['Primary Type'].isin(['BATTERY', 'ASSAULT', 'ROBBERY'])]\n",
    "\n",
    "# Aggregated hourly counts by Primary Type for violent crimes\n",
    "hourly_violent = selected_violent.groupby(['Hour', 'Primary Type']).size().reset_index(name='Count')\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.lineplot(data=hourly_violent, x='Hour', y='Count', hue='Primary Type', marker='o')\n",
    "plt.title('Hourly Patterns for Selected Violent Crime Types')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Number of Crimes')\n",
    "plt.xticks(range(0, 24))\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated counts by day of the week (0=Monday, 6=Sunday)\n",
    "dow_counts = df_vp.groupby(['DayOfWeek', 'Crime Category']).size().reset_index(name='Count')\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.lineplot(data=dow_counts, x='DayOfWeek', y='Count', hue='Crime Category', marker='o')\n",
    "plt.title('Day-of-Week Crime Counts by Category (Violent vs. Property)')\n",
    "plt.xlabel('Day of Week (0=Monday, 6=Sunday)')\n",
    "plt.ylabel('Number of Crimes')\n",
    "plt.xticks(range(0,7))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined weekend indicator: Saturday (5) and Sunday (6)\n",
    "df['IsWeekend'] = df['DayOfWeek'].apply(lambda x: 'Weekend' if x in [5, 6] else 'Weekday')\n",
    "\n",
    "# Filtering for violent crimes\n",
    "violent_data = df[df['Crime Category'] == 'Violent']\n",
    "\n",
    "# Aggregate hourly counts by Day Type (Weekday vs. Weekend)\n",
    "hourly_week = violent_data.groupby(['Hour', 'IsWeekend']).size().reset_index(name='Count')\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.lineplot(data=hourly_week, x='Hour', y='Count', hue='IsWeekend', marker='o')\n",
    "plt.title('Hourly Violent Crime Counts: Weekday vs. Weekend')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Number of Violent Crimes')\n",
    "plt.xticks(range(0, 24))\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_vs_non_monthly['Combo'] = res_vs_non_monthly['Area Type'] + '_' + res_vs_non_monthly['Crime Category']\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.lineplot(\n",
    "    data=res_vs_non_monthly,\n",
    "    x='Month',\n",
    "    y='Count',\n",
    "    hue='Combo',    \n",
    "    marker='o'\n",
    ")\n",
    "plt.title('Monthly Crime Counts by Combined Area Type and Crime Category')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Crimes')\n",
    "plt.xticks(range(1, 13))\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined 'Area Type' and 'TimeOfDay' into one column\n",
    "monthly_area_time['AreaTime'] = monthly_area_time['Area Type'] + '_' + monthly_area_time['TimeOfDay']\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.lineplot(\n",
    "    data=monthly_area_time,\n",
    "    x='Month',\n",
    "    y='Count',\n",
    "    hue='AreaTime',\n",
    "    marker='o'  # Remove style parameter to keep the legend simpler\n",
    ")\n",
    "plt.title('Monthly Property Crime Counts by Combined Area Type and Time of Day')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Property Crimes')\n",
    "plt.xticks(range(1,13))\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the DataFrame for property crimes\n",
    "property_data = df[df['Crime Category'] == 'Property']\n",
    "\n",
    "# Aggregated daily property crime counts (group by the date portion of 'Date')\n",
    "daily_counts = property_data.groupby(property_data['Date'].dt.date).size().reset_index(name='DailyCount')\n",
    "daily_counts['Date'] = pd.to_datetime(daily_counts['Date'])\n",
    "daily_counts['Month'] = daily_counts['Date'].dt.month\n",
    "\n",
    "# Now, group daily counts by Month to get multiple observations per month\n",
    "groups = [group['DailyCount'].tolist() for name, group in daily_counts.groupby('Month')]\n",
    "\n",
    "# Performing the ANOVA test on the groups\n",
    "f_stat, p_value = stats.f_oneway(*groups)\n",
    "print(\"ANOVA test for daily property crime counts by month: F =\", f_stat, \", p =\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated daily property crime counts (from earlier)\n",
    "property_data = df[df['Crime Category'] == 'Property']\n",
    "daily_counts = property_data.groupby(property_data['Date'].dt.date).size().reset_index(name='DailyCount')\n",
    "daily_counts['Date'] = pd.to_datetime(daily_counts['Date'])\n",
    "daily_counts['Month'] = daily_counts['Date'].dt.month\n",
    "\n",
    "# Applying Tukey's HSD\n",
    "tukey = pairwise_tukeyhsd(endog=daily_counts['DailyCount'], groups=daily_counts['Month'], alpha=0.05)\n",
    "print(tukey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering daily property counts for two time periods\n",
    "daily_counts['Year'] = daily_counts['Date'].dt.year\n",
    "pre_2010 = daily_counts[daily_counts['Year'] < 2010]\n",
    "post_2010 = daily_counts[daily_counts['Year'] >= 2010]\n",
    "\n",
    "# Grouped daily counts by month for each period\n",
    "groups_pre = [group['DailyCount'].tolist() for name, group in pre_2010.groupby('Month')]\n",
    "groups_post = [group['DailyCount'].tolist() for name, group in post_2010.groupby('Month')]\n",
    "\n",
    "# Performed ANOVA for pre-2010 and post-2010 separately\n",
    "f_stat_pre, p_value_pre = stats.f_oneway(*groups_pre)\n",
    "f_stat_post, p_value_post = stats.f_oneway(*groups_post)\n",
    "\n",
    "print(\"Pre-2010: F =\", f_stat_pre, \", p =\", p_value_pre)\n",
    "print(\"Post-2010: F =\", f_stat_post, \", p =\", p_value_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated daily crime counts\n",
    "daily_counts = df.groupby(['Year', 'Month', 'Day']).size().reset_index(name='Crime_Count')\n",
    "\n",
    "# Created a Date column from Year, Month, and Day and extract Day of Week\n",
    "daily_counts['Date'] = pd.to_datetime(daily_counts[['Year','Month','Day']])\n",
    "daily_counts['DayOfWeek'] = daily_counts['Date'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "\n",
    "# Selected features and target\n",
    "features = ['Year', 'Month', 'Day', 'DayOfWeek']\n",
    "X = daily_counts[features]\n",
    "y = daily_counts['Crime_Count']\n",
    "\n",
    "# Splited data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialized and train XGBoost regressor\n",
    "model_xgb = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "model_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = model_xgb.predict(X_test)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred)\n",
    "print(\"XGBoost Mean Squared Error:\", mse_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted daily_counts by Date\n",
    "daily_counts.sort_values(by='Date', inplace=True)\n",
    "data = daily_counts['Crime_Count'].values.reshape(-1, 1)\n",
    "\n",
    "# Scale the data to the range [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Function to create sequences for time-series prediction\n",
    "def create_sequences(data, seq_length=7):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X_seq.append(data[i:i+seq_length])\n",
    "        y_seq.append(data[i+seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Create sequences (using 7 days of data to predict the next day)\n",
    "seq_length = 7\n",
    "X_seq, y_seq = create_sequences(data_scaled, seq_length)\n",
    "\n",
    "# Split into training and testing sets (80/20 split)\n",
    "split_index = int(0.8 * len(X_seq))\n",
    "X_train_seq, X_test_seq = X_seq[:split_index], X_seq[split_index:]\n",
    "y_train_seq, y_test_seq = y_seq[:split_index], y_seq[split_index:]\n",
    "\n",
    "# Define the LSTM model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(50, activation='relu', input_shape=(seq_length, 1)))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model_lstm.fit(X_train_seq, y_train_seq, epochs=20, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Predict on the test set and invert scaling\n",
    "predictions = model_lstm.predict(X_test_seq)\n",
    "predictions_inv = scaler.inverse_transform(predictions)\n",
    "y_test_inv = scaler.inverse_transform(y_test_seq)\n",
    "\n",
    "# Compute Mean Squared Error for LSTM\n",
    "mse_lstm = np.mean((predictions_inv - y_test_inv)**2)\n",
    "print(\"LSTM Mean Squared Error:\", mse_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated daily crime counts (if not already done)\n",
    "# Using the 'Date' column and count the number of crimes per day.\n",
    "daily_counts = df.groupby(df['Date'].dt.date).size().reset_index(name='Crime_Count')\n",
    "daily_counts['ds'] = pd.to_datetime(daily_counts['index'] if 'index' in daily_counts.columns else daily_counts['Date'])\n",
    "daily_counts.rename(columns={'Crime_Count': 'y'}, inplace=True)\n",
    "daily_counts = daily_counts[['ds', 'y']]\n",
    "\n",
    "# Sorted the data by date\n",
    "daily_counts.sort_values('ds', inplace=True)\n",
    "\n",
    "# Initialized the Prophet model\n",
    "model_prophet = Prophet(daily_seasonality=True)\n",
    "model_prophet.fit(daily_counts)\n",
    "\n",
    "# Created a dataframe to hold predictions for the next 30 days\n",
    "future = model_prophet.make_future_dataframe(periods=30)\n",
    "forecast = model_prophet.predict(future)\n",
    "\n",
    "# Plot the forecast\n",
    "fig1 = model_prophet.plot(forecast)\n",
    "plt.title(\"Daily Crime Count Forecast using Prophet\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Crime Count\")\n",
    "plt.show()\n",
    "\n",
    "# Plot forecast components (trend, weekly seasonality, yearly seasonality)\n",
    "fig2 = model_prophet.plot_components(forecast)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "\n",
    "df_cv = cross_validation(model_prophet, initial='730 days', period='180 days', horizon='365 days')\n",
    "df_metrics = performance_metrics(df_cv)\n",
    "print(df_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation:\n",
    "# - initial: period to use for training initially (e.g., 3 years: 1095 days)\n",
    "# - period: spacing between cutoff dates (e.g., every 180 days)\n",
    "# - horizon: how far into the future to forecast (e.g., 365 days)\n",
    "df_cv = cross_validation(model_prophet, initial='1095 days', period='180 days', horizon='365 days')\n",
    "\n",
    "# Compute performance metrics (MAE, RMSE, MAPE, etc.)\n",
    "df_performance = performance_metrics(df_cv)\n",
    "print(df_performance.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting the forecast\n",
    "fig = model_prophet.plot(forecast)\n",
    "plt.title(\"Forecast of Daily Crime Counts in Chicago\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Daily Crime Count\")\n",
    "\n",
    "# Added a vertical line to mark the end of historical data\n",
    "last_date = daily_counts['ds'].max()  # the last date in your training data\n",
    "plt.axvline(x=last_date, color='red', linestyle='--', label='Forecast Start')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_components = model_prophet.plot_components(forecast)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the forecasted values for the next 30 days\n",
    "future_forecast = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(30)\n",
    "print(future_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a future dataframe for 2 more years (730 days)\n",
    "future = model_prophet.make_future_dataframe(periods=730)  \n",
    "forecast = model_prophet.predict(future)\n",
    "\n",
    "# Plot the extended forecast\n",
    "fig = model_prophet.plot(forecast)\n",
    "plt.title(\"Forecast of Daily Crime Counts in Chicago\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Daily Crime Count\")\n",
    "\n",
    "# Mark the end of the historical data\n",
    "last_date = daily_counts['ds'].max()\n",
    "plt.axvline(x=last_date, color='red', linestyle='--', label='Forecast Start')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Inspecting the last few rows of the forecast\n",
    "print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a working copy for classification\n",
    "clf_df = df.copy()\n",
    "\n",
    "# Encode categorical features: \"Primary Type\" and \"Area Type\"\n",
    "le_primary = LabelEncoder()\n",
    "clf_df['Primary_Type_enc'] = le_primary.fit_transform(clf_df['Primary Type'])\n",
    "\n",
    "le_area = LabelEncoder()\n",
    "clf_df['Area_Type_enc'] = le_area.fit_transform(clf_df['Area Type'])\n",
    "\n",
    "# Select features and target\n",
    "# We use: Primary_Type_enc, Domestic (bool), District, Month, Hour, DayOfWeek, Area_Type_enc\n",
    "features = ['Primary_Type_enc', 'Domestic', 'District', 'Month', 'Hour', 'DayOfWeek', 'Area_Type_enc']\n",
    "target = 'Arrest'\n",
    "\n",
    "X = clf_df[features]\n",
    "y = clf_df[target].astype(int)  # Convert boolean to integer (0 or 1)\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train XGBoost Classifier\n",
    "model_xgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42,\n",
    "                                  use_label_encoder=False, eval_metric='logloss')\n",
    "model_xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate performance\n",
    "y_pred = model_xgb_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, model_xgb_clf.predict_proba(X_test)[:,1])\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Extract feature importances\n",
    "importances = model_xgb_clf.feature_importances_\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "importance_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Importance Score')\n",
    "plt.title('Feature Importances for Predicting Arrest')\n",
    "plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by \"Primary Type\" and calculate total crimes and arrests\n",
    "arrest_rate_df = df.groupby('Primary Type').agg(\n",
    "    total_crimes = ('Arrest', 'count'),\n",
    "    arrests = ('Arrest', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate the arrest rate (as a percentage)\n",
    "arrest_rate_df['Arrest_Rate'] = (arrest_rate_df['arrests'] / arrest_rate_df['total_crimes']) * 100\n",
    "\n",
    "# Sort the DataFrame by arrest rate in descending order\n",
    "arrest_rate_df.sort_values(by='Arrest_Rate', ascending=False, inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(arrest_rate_df[['Primary Type', 'total_crimes', 'arrests', 'Arrest_Rate']])\n",
    "\n",
    "# Plot the arrest rate for each crime type\n",
    "plt.figure(figsize=(14,8))\n",
    "sns.barplot(data=arrest_rate_df, x='Arrest_Rate', y='Primary Type', palette='viridis')\n",
    "plt.xlabel(\"Arrest Rate (%)\")\n",
    "plt.ylabel(\"Primary Crime Type\")\n",
    "plt.title(\"Arrest Rate by Crime Type\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
